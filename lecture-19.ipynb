{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "zTHHOovjM1ym",
    "outputId": "fdf29910-1268-457f-a91e-cfa6b1db65a2"
   },
   "outputs": [],
   "source": [
    "!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvZNupW5M1ys"
   },
   "source": [
    "# Lecture 19: Maximum likelihood inference\n",
    "\n",
    "[Maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) is a general approach for estimating the parameters, $\\theta$, of a model from data, $X$. We do this by first writing down the probability of the data as a function of the parameters -- this is called the **likelihood**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X|\\theta)\\,.\n",
    "$$\n",
    "\n",
    "Even in cases where the underlying model is deterministic, the measurements that give us the data are often stochastic. For example, we might estimate the fraction of people who are left handed by a survey of a small subset of the population.\n",
    "\n",
    "In maximum likelihood inference, we search for the parameters $\\hat{\\theta}$ that maximize $\\mathcal{L}(X|\\theta)$. These are the parameters that, out of all the different possibilities, are the ones that are **most likely** to have produced the data $X$ that we observe.\n",
    "\n",
    "\n",
    "### Example: Estimating the magnetic field coupled to a single spin\n",
    "\n",
    "For an example, let's return to the case of the Ising model that we considered several weeks ago. To make things simple, let's consider just a single spin in an external magnetic field. Let's write the spin variable as $\\sigma \\in\\{\\pm 1\\}$, and the energy\n",
    "\n",
    "$$\n",
    "E(\\sigma) = -\\epsilon \\sigma\\,.\n",
    "$$\n",
    "\n",
    "Here $\\epsilon$ is the interaction energy -- the product of the external magnetic field and the magnetic moment of the spin itself -- which we would like to estimate.\n",
    "\n",
    "In thermal equilibrium, the probability that the spin is in either state should be given by the Gibbs distribution\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\sigma) &= \\frac{e^{-E(\\sigma)/T}}{Z}\\,, \\\\\n",
    "Z &= e^{-E(-1)/T} + e^{-E(1)/T}\\,,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we've chosen units such that Boltzmann's constant $k_B=1$. First, let's code the Gibbs distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "WESNiRsEM1yt",
    "outputId": "299344dd-ca92-4dcd-c91b-9f272d6a3cea"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gibbs(eps, T):\n",
    "    \"\"\" This function takes the energy eps and temperature T as input\n",
    "        and returns the Gibbs distribution for a single spin as output \"\"\"\n",
    "    \n",
    "    Z     = np.exp(-eps/T) + np.exp(eps/T)\n",
    "    p_pos = np.exp( eps/T) / Z\n",
    "    p_neg = np.exp(-eps/T) / Z\n",
    "    \n",
    "    return p_pos, p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwrhSycWM1yw"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jj--50KQM1yz"
   },
   "source": [
    "### Generating sample data\n",
    "\n",
    "Next, let's imagine that we make 100 observations of the system and measure the direction of the spin. We can sample those observations from the Gibbs distribution. We'll set $\\epsilon=0.1$ and take $T=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gVNZOrY5M1y0",
    "outputId": "9920099c-9970-4f5b-893a-f6d7dc5c4527"
   },
   "outputs": [],
   "source": [
    "import numpy.random as rng\n",
    "\n",
    "# FILL THIS IN\n",
    "\n",
    "spin_measurements = # FILL THIS IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGBEU7v8M1y3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPI4h6vGM1y6"
   },
   "source": [
    "### Computing the likelihood\n",
    "\n",
    "How likely are different values for $\\epsilon$ to have generated this data? This is given by the likelihood function $\\mathcal{L}$. Here, all the measurements of the spin direction are independent, so the likelihood is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\{\\sigma_i\\}|\\epsilon) = \\prod_{i=1}^{100} P_\\epsilon(\\sigma_i)\\,.\n",
    "$$\n",
    "\n",
    "In the equation above, $\\sigma_i$ refers to the $i$th measurement of the spin, and $P_\\epsilon(\\sigma)$ denotes the probability of the spin being in state $\\sigma$ given that the interaction energy is $\\epsilon$.\n",
    "\n",
    "With a large number of observations, the likelihood may be challenging to work with numerically because it rapidly becomes very small. Often we work with the logarithm of the likelihood, referred to as the log-likelihood, instead. Because the logarithm is a monotonic function, maximizing the log-likelihood $\\ell$ is equivalent to maximizing the likelihood $\\mathcal{L}$. The expression for the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell (\\{\\sigma_i\\}|\\epsilon) = \\log\\left[\\mathcal{L}(\\{\\sigma_i\\}|\\epsilon)\\right] = \\sum_{i=1}^{100} \\log\\left(P_\\epsilon(\\sigma_i)\\right)\\,.\n",
    "$$\n",
    "\n",
    "Now, instead of a product of many terms, we have a sum. Below, let's compute the log-likelihood and plot it as a function of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "lERitbWsM1y7",
    "outputId": "f68bd8dd-1fa2-48ff-b719-cdbe91bcad18"
   },
   "outputs": [],
   "source": [
    "def get_ell(p_pos, p_neg, data):\n",
    "    return # FILL THIS IN\n",
    "\n",
    "\n",
    "eps_vals = np.arange(-2, 2, 0.05)\n",
    "ell_vals = # FILL THIS IN\n",
    "\n",
    "\n",
    "sns.lineplot(eps_vals, ell_vals)\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('$\\ell$')\n",
    "plt.title('Log-likelihood versus epsilon');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frJWTXqrM1y9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "th5En8gYM1zA"
   },
   "source": [
    "### Estimating the interaction energy\n",
    "\n",
    "**Exercise:** using the formula that we derived, fill in the code below to derive the maximum likelihood estimate of $\\epsilon$ for a given set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ffClai9M1zB"
   },
   "outputs": [],
   "source": [
    "def get_eps(data):\n",
    "    return # FILL THIS IN\n",
    "\n",
    "print('For this data, the estimated epsilon = %.2e, true value = %.2e' % (get_eps(spin_measurements), eps_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxXiftxpM1zE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d34icHWM1zH"
   },
   "source": [
    "### Robustness to finite sampling\n",
    "\n",
    "How stable are our estimates, depending on the amount of data that we have? Below, we'll experiment with using smaller or larger sample sizes. We can also test these results with different values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "De5hK_4_M1zI"
   },
   "outputs": [],
   "source": [
    "# FILL THIS IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1IvAVByM1zL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3EjR2CTM1zO"
   },
   "source": [
    "### Finding the parameters using optimization\n",
    "\n",
    "In this simple case we can derive an analytical expression for the maximum likelihood estimate of $\\epsilon$, but in more general cases this is usually impossible. Instead, we may need to numerically maximize the likelihood. Here, we will apply the steepest descent algorithm (with line search) to this problem. Note that we need to rearrange our problem in order to **minimize** the **negative** of the log-likelihood with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xrs3L6JRM1zP"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" Return minus the log-likelihood of the data at x = epsilon. \"\"\"\n",
    "    p_pos, p_neg = gibbs(x, 1)\n",
    "    return -np.sum(spin_measurements==1)*np.log(p_pos) -np.sum(spin_measurements==-1)*np.log(p_neg)\n",
    "\n",
    "def df(x):\n",
    "    \"\"\" Return the derivative of minus the log-likelihood at x = epsilon. \"\"\"\n",
    "    p_pos, p_neg = gibbs(x, 1)\n",
    "    return -np.sum(spin_measurements) + len(spin_measurements)*(p_pos - p_neg)\n",
    "\n",
    "\n",
    "# Set line search parameters\n",
    "\n",
    "beta1 = 0.4    # Step size multiplier if sufficient decrease fails\n",
    "beta2 = 1.2    # Step size multiplier if curvature condition fails\n",
    "alpha = 0.001  # Sufficient decrease parameter\n",
    "gamma = 0.5    # Curvature condition parameter\n",
    "\n",
    "\n",
    "# Set steepest descent parameters\n",
    "\n",
    "epsilon  = 0.001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "max_iter = 100    # Maximum number of iterations\n",
    "it       = 0      # Current iteration\n",
    "\n",
    "\n",
    "# Initialize and iteratre\n",
    "\n",
    "x0   = 0      # Starting value of parameter\n",
    "x    = x0     # Current value of the parameter\n",
    "dfdx = df(x0) # Starting value of the derivative df/dx\n",
    "\n",
    "# Report status\n",
    "print('iter\\tx\\tf(x)\\tdf/dx')\n",
    "\n",
    "# Now loop through the steepest descent algorithm\n",
    "\n",
    "while np.fabs(dfdx)>=epsilon and it<max_iter:\n",
    "    \n",
    "    # Report status\n",
    "    print('%d\\t%.4f\\t%.4f\\t%.4f' % (it, x, f(x), dfdx))\n",
    "    \n",
    "    # Choose the step direction\n",
    "    s = -df(x)\n",
    "    \n",
    "    # Choose how far to step in that direction\n",
    "    t = 1 \n",
    "    both_passed = False\n",
    "    \n",
    "    while not both_passed:\n",
    "\n",
    "        # Check for sufficient decrease fail\n",
    "\n",
    "        if f(x + (t*s)) > f(x) + (alpha * t * np.dot(df(x), s)):\n",
    "            print('\\tSufficient decrease failed, reducing t %lf --> %lf' % (t, beta1*t))\n",
    "            t = beta1 * t\n",
    "\n",
    "        # If passed, check for curvature condition fail\n",
    "\n",
    "        elif np.dot(df(x + (t*s)), s) < gamma * np.dot(df(x), s):\n",
    "            print('\\tCurvature condition failed, increasing t %lf --> %lf' % (t, beta2*t))\n",
    "            t = beta2 * t\n",
    "\n",
    "        # If both passed, exit the loop\n",
    "\n",
    "        else:\n",
    "            both_passed = True\n",
    "\n",
    "    print('\\tThe accepted step length is %lf' % t)\n",
    "    \n",
    "    # Update the parameters\n",
    "    x = x + t*s\n",
    "    \n",
    "    # Update the derivative\n",
    "    dfdx = df(x)\n",
    "    \n",
    "    # Update the iteration counter\n",
    "    it += 1\n",
    "    \n",
    "# Report the minimum\n",
    "\n",
    "true_min = np.arctanh((np.sum(spin_measurements==1)-np.sum(spin_measurements==-1)) / len(spin_measurements))\n",
    "\n",
    "print('\\nFound the minimum near x* = %lf, true minimum is %lf' % (x, true_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GFzBeFPM1zX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of lecture-17.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
